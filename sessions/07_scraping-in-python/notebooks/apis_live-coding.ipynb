{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f86c135-826b-43c6-b952-129fc35de05f",
   "metadata": {},
   "source": [
    "## Using API's in Python\n",
    "\n",
    "### How to use API's\n",
    "\n",
    "API's are very different but usually consist of the following components:\n",
    "\n",
    "**Request:** Like any other interactions with the web, using API's involves sending a request (GET or POST) to the API.\n",
    "\n",
    "**Endpoint:** API's usually consists of different endpoints. These can be considered different outlets. Endpoints are simply URLs we send the request to.\n",
    "\n",
    "**Parameters:** Parameters are the arguements the endpoint accepts. Some may be required, others are optionals.\n",
    "\n",
    "**Authentication:** Most API's requires some kind of authentication. This can be either HTTPS authentication (username and password) or authentication via tokens. Tokens are essentially unique keys that identify who is making the request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810dd0a-3531-4b4d-9348-56994c4eb388",
   "metadata": {},
   "source": [
    "## Example: Using the Statistics Denmark’s API for StatBank\n",
    "\n",
    "Link to API documentation: https://www.dst.dk/en/Statistik/brug-statistikken/muligheder-i-statistikbanken/api\n",
    "\n",
    "The Statistics Denmark's API for StatBank makes it possible to access the data in Statbank.\n",
    "\n",
    "The following demonstrates how to interact with the API directly via python.\n",
    "\n",
    "*Note*: The StatBank API does not require authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef74b0-fd24-45ff-92f7-20da71b79970",
   "metadata": {},
   "source": [
    "### Extracting data from the StatBank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8dd46-5eae-470b-b410-8bae415ca331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "statbank_api = \"https://api.statbank.dk/v1/data\"  #Endpoint of the data API\n",
    "\n",
    "data_req = {'table': 'folk1c',\n",
    "            'format': 'CSV',\n",
    "            'variables': [{'code': 'OMRÅDE', 'values': ['101', '851']},  #Request in JSON/dictionary\n",
    "                                                            {'code': 'ALDER', 'values': ['20-24', '25-29']}]\n",
    "           }\n",
    "\n",
    "data_req = requests.post(statbank_api, json=data_req)  #Sending requests\n",
    "\n",
    "print(data_req.text)  #Printing the raw text output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3997c303-54c7-483a-aec1-6c0ebb59a781",
   "metadata": {},
   "source": [
    "The data API returns commma-separated values by default (csv).\n",
    "\n",
    "This output is directly readable by the `pandas` package (`pd.read_csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66a65c-4e16-48db-8135-e477a21a378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "dstdata = StringIO(data_req.text)  #Read the data output as raw text\n",
    "dstdf = pd.read_csv(dstdata, sep=\";\")  #Read text as csv\n",
    "dstdf  #Print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1016f2-3144-4098-a2d0-e863d3a656f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dstdf.groupby(['OMRÅDE']).sum()  #Group by municipality and count sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0dde6-daee-4ce0-914d-a1656e78d0c5",
   "metadata": {},
   "source": [
    "## Example: Using the Pushshift API for Reddit data\n",
    "\n",
    "Link to API documentation: https://github.com/pushshift/api\n",
    "\n",
    "The Pushshift API provides endpoints for extracting submission and comments from continously updated datasets of the entirety of reddit.com.\n",
    "\n",
    "There are two main endpoints used to search all publicly available comments and submissions on Reddit:\n",
    "- https://api.pushshift.io/reddit/search/comment/\n",
    "- https://api.pushshift.io/reddit/search/submission/\n",
    "\n",
    "Searching comments for a specific submission, requires retrieving the *comment ids* for a submission. These can be retrieved via the endpoint:\n",
    "- https://api.pushshift.io/reddit/submission/comment_ids/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748300e-845a-4638-8a5f-8573e0f6ed12",
   "metadata": {},
   "source": [
    "### Searching for submissions in a subreddit\n",
    "\n",
    "In the code below, we extract data on submissions from X. We provide the API the following parameters:\n",
    "- The subreddit to search for (via the `subreddit`)\n",
    "- A search query (via the  `q` parameter)\n",
    "- A timeframe (via the `before` and `after` parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babbcdba-07f5-422e-80d3-1bf908ecf012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "endpoint = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "    \n",
    "subreddit = 'denmark'\n",
    "start_time = int(datetime(2022,9,21,0,0).timestamp())\n",
    "end_time = int(datetime(2022,9,28,0,0).timestamp())\n",
    "q = 'mette'\n",
    "\n",
    "params = {'subreddit': subreddit,\n",
    "          'after': start_time,\n",
    "          'before': end_time,\n",
    "          'size': 499,\n",
    "          'q': q\n",
    "         }\n",
    "\n",
    "r = requests.get(endpoint, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2e03e-bba0-43c9-a143-8f1633967534",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e739a-3973-473a-a27e-cb603a1f47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = r.json().get('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48b5ee-3e22-4aed-aedd-6c52fa24a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a780845-ff78-41e3-9d0e-6ca657123175",
   "metadata": {},
   "source": [
    "### Collecting comments from a submission\n",
    "\n",
    "To collect comments, we first need the comment ids. In the code below, we first extract the submission id for a submission collected. Afterwards we retrieve the comment ids using the \"comment_ids\" endpoint. Finally, we retrieve comment data using the \"comment\" endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635143a-f831-47c4-a3c3-b39cf3a4d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcomment_end = \"https://api.pushshift.io/reddit/submission/comment_ids/\" # endpoint for getting comment id\n",
    "\n",
    "subid = submissions[5].get('id') # get id for submission 5 in the collected submissions\n",
    "\n",
    "request_url = f\"{subcomment_end}{subid}\" # API call via URL as f-string (the two strings above basically pasted together)\n",
    "\n",
    "r = requests.get(request_url) # send request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252628e2-2b60-4add-b5d0-e002df6ce14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_ids = r.json().get('data') # collect comment ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b12b6-b059-420e-b481-fcbeb1754223",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_end = \"https://api.pushshift.io/reddit/search/comment/\" # endpoint for comment data\n",
    "\n",
    "params = {'ids': comment_ids} # set parameters - the comment ids to collect\n",
    "       \n",
    "r = requests.get(comment_end, params = params) # API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2bb540-8051-4f29-85ca-3fd492e76c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = r.json().get('data') # get comment from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264eaa51-a436-4b94-94b5-77be383484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98a4c5-d46e-4432-899f-e9dda8e49dff",
   "metadata": {},
   "source": [
    "## Example: Using the Twitter API\n",
    "\n",
    "***NOTE***: This notebook uses a token that is not included in the notebook. You will not be able to reproduce this on your own computer without proper authentication (for this you need access to the Twitter enterprise API: https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api\n",
    "\n",
    "The Twitter API contains a wide variety of endpoints for both interacting with Twitter (sending tweets, replying) and for retrieving data.\n",
    "\n",
    "The example below uses the \"Search Tweets\" endpoints (full archive search): https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all. The example retrieves tweets from Elon Musk from the last week.\n",
    "\n",
    "It is adapted from Twitter's own sample code: https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/Full-Archive-Search/full-archive-search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9eae8-4091-454c-878b-ad4f290df59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# token and endpoint\n",
    "with open(os.path.join(\"C:/\", \"repos\", \"tokens\", \"twitter_bearer.txt\"), 'r') as f:\n",
    "    bearer_token = f.read()\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "# set start_time\n",
    "d = datetime.today() - timedelta(days=7)\n",
    "start_time = f\"{str(d.date())}T00:00:00Z\"\n",
    "\n",
    "query_params = {'query': 'from:elonmusk -is:retweet',\n",
    "                'tweet.fields': 'entities,public_metrics,created_at,referenced_tweets',\n",
    "                'expansions': 'author_id',\n",
    "                'user.fields': 'created_at,description,public_metrics,url,verified', \n",
    "                'max_results': 500,\n",
    "                'start_time': start_time}\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FullArchiveSearchPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(search_url, auth=bearer_oauth, params=params)\n",
    "    #print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def initial():\n",
    "    json_response = connect_to_endpoint(search_url, query_params)\n",
    "    return(json_response)\n",
    "\n",
    "def continued(next_token):\n",
    "    new_params = query_params.copy()\n",
    "    new_params['next_token'] = next_token\n",
    "    json_response = connect_to_endpoint(search_url, new_params)\n",
    "    return(json_response)\n",
    "\n",
    "data = initial()\n",
    "all_data = data.copy()\n",
    "all_data.pop('meta', None)\n",
    "\n",
    "used_next_tokens = []\n",
    "next_token = data.get('meta').get('next_token')\n",
    "\n",
    "if next_token is not None:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        data = continued(next_token)\n",
    "        all_data['data'] = all_data.get('data') + data.get('data')\n",
    "        all_data['includes']['users'] = all_data.get('includes').get('users') + data.get('includes').get('users')\n",
    "\n",
    "        used_next_tokens.append(next_token)\n",
    "\n",
    "        next_token = data.get('meta').get('next_token')\n",
    "\n",
    "        if next_token is None:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
