{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c2bcb8-694c-4ef3-882a-12d8ec1110b6",
   "metadata": {},
   "source": [
    "## Simple tokenizers\n",
    "\n",
    "Der findes en lang række eksisterende tokenizers fra forskellige pakker i Python. Disse kan med fordel bruges frem for at skrive sin egen tokenizer fra bunden.\n",
    "\n",
    "SpaCy's sprogmodeller indeholder altid en tokenizer. Hvis man blot vil bruge selve tokenizeren uden de andre \"processors\" i modellen, kan man gøre det lig nedenstående:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1745b686-f3f2-459f-9ab0-07b40543b4a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy # importer pakke\n",
    "\n",
    "nlp = spacy.load('da_core_news_sm') # indlæs sprogmodel\n",
    "\n",
    "tokenizer = nlp.tokenizer # hiv tokenizer ud af model (funktion for sig)\n",
    "\n",
    "text = 'Rutefly måtte lande i New York, fordi piloten døde' # tekststykke\n",
    "tokens = list(tokenizer(text)) # brug tokenizer - her liste af tokens\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df6ed7-de7e-44b3-8dc0-03839d8afddf",
   "metadata": {},
   "source": [
    "## Udvidet tokenizer\n",
    "\n",
    "Man kan med fordel udnytte de forskellige måder, som spaCy's sprogmodeller behandler og analyserer tekst. \n",
    "\n",
    "I nedenstående defineres en udvidet tokenizerfunktion. Bemærk, hvordan de enkelte elementer af funktionen kan rettes til (hvilke stopord, hvilke part-of-speech tags, token længe, hvorvidt der skal bruges lemma eller ej)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b497d7-72bb-4774-bb6c-47ffcf90acc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \n",
    "    custom_stops = [\"\"] # Definerer kontekstspecifikke stopord\n",
    "    default_stopwords = list(nlp.Defaults.stop_words) # Indlæser prædefineret stopordsliste\n",
    "    stop_words = default_stopwords + custom_stops # Danner samlet stopordsliste\n",
    "    \n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN', 'VERB'] # Definerer POS-tags som skal bevares\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for word in doc: # Looper igennem hvert ord i tweet\n",
    "        if (len(word.lemma_) < 3): # Ord må ikke være mindre end 3 karakterer - går videre til næste ord, hvis det er\n",
    "            continue\n",
    "        if (word.pos_ in pos_tags) and (word.lemma_ not in stop_words): # Tjek at ordets POS-tag indgår i listen af accepterede tags og at ordet ikke er stopord\n",
    "            tokens.append(word.lemma_) # Tilføj ordets lemma til tokens, hvis if-betingelse er opfyldt\n",
    "                \n",
    "    return(tokens)\n",
    "\n",
    "text = 'Rutefly måtte lande i New York, fordi piloten døde' # tekststykke\n",
    "tokens = tokenizer(text) # brug tokenizer - her liste af tokens\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424f9f2-c770-452b-aa3f-bfe0140be6fa",
   "metadata": {},
   "source": [
    "## Vectorizers\n",
    "\n",
    "`sklearn` indeholder forskellige \"text-vectorization\"-funktioner; altså funktioner, der omdanner tekststykker til matematiske repræsentationer.\n",
    "\n",
    "`CountVectorizer()` laver simple ordtællinger. Vectorizers fra `sklearn` bruges typisk på følgende måde:\n",
    "\n",
    "1. Definér vectorizerfunktion (hvad skal funktionen, hvilke parametre)\n",
    "2. Brug/\"fit\" funktionen på tekster/corpus (gerne som liste af strings)\n",
    "3. Konvertér til passende datastruktur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ccbfd-aa23-4050-a707-9b76c2c372c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = ['Forbrugerombudsmanden anmelder influencere til politiet',\n",
    "         'Politiet pågriber for brandattentat i Hellerup',\n",
    "         'Politiet tavs om motivet bag brandstiftelse i Hellerup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3123818-32f7-420b-a4ef-726e02b5ad31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer # importer CountVectorizer funktion\n",
    "\n",
    "vectorizer = CountVectorizer() # dan vectorizerfunktion\n",
    "transformed_documents = vectorizer.fit_transform(texts) # brug vectorizer på tekster\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()# Konverter fittet vectorizer til array\n",
    "\n",
    "count_df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names_out()) # Konverter til data frame\n",
    "\n",
    "count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efe034-e045-45a6-a9ab-9c0885691450",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "`sklearn` har også en tf-idf vectorizer. Denne laver tf-idf vægtning af ord i teksten. \n",
    "\n",
    "*Bemærk:* Som standard normaliserer funktionen vægtene (kvadratsummen af vægte for hver tekst = 1 - se evt.: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e4552-b58f-4ec6-93c7-24495cd7bd9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer() # dan vectorizerfunktion\n",
    "transformed_documents = vectorizer.fit_transform(texts) # brug vectorizer på tekster\n",
    "\n",
    "# Konverter fittet vectorizer til array\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Konverter til data frame\n",
    "tfidf_df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfed577-b647-414e-8eb4-c608ebca5f18",
   "metadata": {},
   "source": [
    "### Tilpasning af vectorizer - egen tokenizer\n",
    "\n",
    "Vectorizer-funktionerne fra `sklearn` kan tilpasses på en lang række forskellige måder. En måde, som man kan tilpasse funktionerne på, er ved at bruge sin egen tokenizer. \n",
    "\n",
    "For at gøre dette, skal man blot have defineret en funktion, der omdanner et enkelt tekststykke til tokens (lig funktionen defineret tidligere i denne notebook). Denne sættes så ind som tokenizer, når man danner vectorizerfunktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4767832-4c67-48b0-b0a7-887d99a65d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = tokenizer) # dan vectorizerfunktion med egen tokenizer\n",
    "transformed_documents = vectorizer.fit_transform(texts) # brug vectorizer på tekster\n",
    "\n",
    "# Konverter fittet vectorizer til array\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Konverter til data frame\n",
    "tfidf_df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd31ef-72cc-4c98-b0fc-d23fa6358746",
   "metadata": {},
   "source": [
    "### Tilpasning af vectorizer - dokumentgrænser\n",
    "\n",
    "En anden måde, som man kan tilpasse vectorizers, er ved at sætte minimum- og maksimumsgrænser for, hvor mange dokumenter, som ord skal indgå i.\n",
    "\n",
    "Argumenterne `min_df` og `max_df` bruges til at sætte grænser for hhv. minimum antal dokumenter og maksimum antal dokumenter, som ord skal indgå i (df = \"document frequency\"). Ved begge kan man både angive absolutte tal eller andel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e702818-9142-4aba-83fb-b8976422c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 2) # dan vectorizerfunktion med egen tokenizer - ord skal indgå i to dokumenter\n",
    "transformed_documents = vectorizer.fit_transform(texts) # brug vectorizer på tekster\n",
    "\n",
    "# Konverter fittet vectorizer til array\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "# Konverter til data frame\n",
    "tfidf_df = pd.DataFrame(transformed_documents_as_array, columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "tfidf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
