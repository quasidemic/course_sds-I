{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brug af dictionary metoder og sentiment analyse i Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary metoder i Python\n",
    "\n",
    "Inden for computationel tekstanalyse, har man længe brugt såkaldte \"dictionary metoder\" eller \"dictionary modeller\" (har i øvrigt ikke noget med Python dictionaries at gøre). \n",
    "\n",
    "Med tiden er modellerne blevet mere komplekse og sofistikerede, men i deres mest basale form, er det blot modeller, der tildeler en stykke tekst en score baseret ud fra en række prædefinerede ord (en \"dictionary\"). \n",
    "\n",
    "Man kan finde forskellige pakker med prædefinerede dictionary modeller (fx til sentiment analysis), men man kan også lave sine egne.\n",
    "\n",
    "Herunder ses to eksempler på en dictionary-tilgang med basis Python funktionalitet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpel dictionary\n",
    "\n",
    "Herunder et eksempel med en simpel dictionary, der blot optæller, hvor mange gange visse ord optræder i hver tekst.\n",
    "\n",
    "Der bruges en række nyheder/blogindlæg fra [sf.dk](https://sf.dk/) som eksempeldata (hentet efteråret 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indlæs pakker\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indlæs data\n",
    "\n",
    "data_p = '/work/teaching-materials/data/polparties/sf'\n",
    "\n",
    "# sf texts\n",
    "sf_texts = []\n",
    "\n",
    "sf_files = os.listdir(data_p)\n",
    "\n",
    "for filename in sf_files:\n",
    "    if filename.endswith('.txt'):\n",
    "        textfile = join(data_p, filename)\n",
    "\n",
    "        with open(textfile, 'r', encoding = 'utf-8') as f:\n",
    "            text = f.read()\n",
    "    \n",
    "        sf_texts.append(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definér keywords (\"ordbog\")\n",
    "\n",
    "keywords = ['skat', 'finans', 'økonomi', 'penge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpel funktion der scorer tekst baseret på keywords optræden\n",
    "\n",
    "def simple_dict(text, keywords = keywords):\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        word_as_regex = re.compile(keyword)\n",
    "        \n",
    "        n_found = len(word_as_regex.findall(text_lower))\n",
    "\n",
    "        score = score + n_found\n",
    "\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brug funktion på første 10 tekster\n",
    "\n",
    "for text in sf_texts[0:10]:\n",
    "\n",
    "    print(simple_dict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konvertér til pandas series\n",
    "texts_series = pd.Series(sf_texts)\n",
    "\n",
    "# brug funtkion på series af tekster\n",
    "scores_series = texts_series.apply(simple_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram af scores\n",
    "scores_series.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vægtet dictionary\n",
    "\n",
    "I nedenstående ses eksempel på brug af en vægtet dictionary, hvor hvert nøgleord gives en score. Igen bruges SF-teksterne som eksempeldata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scored keywords\n",
    "\n",
    "scored_keywords = {\n",
    "    'skat': 3, \n",
    "    'finans': 3, \n",
    "    'økonomi': 2, \n",
    "    'penge': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary funktion der scorer tekster pba. keywords og deres scores\n",
    "\n",
    "def scored_dict(text, keywords = scored_keywords):\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for keyword, weight in keywords.items():\n",
    "        word_as_regex = re.compile(keyword)\n",
    "        \n",
    "        n_found = len(word_as_regex.findall(text_lower))\n",
    "\n",
    "        n_weighted = n_found * weight # gang antal fundne ord med score for nøgleord (vægt)\n",
    "        \n",
    "        score = score + n_weighted # opdater score\n",
    "\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konvertér til pandas series\n",
    "texts_series = pd.Series(sf_texts)\n",
    "\n",
    "# brug funtkion på series af tekster\n",
    "scores_series = texts_series.apply(scored_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram af scores\n",
    "scores_series.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analyse med `asent` i Python\n",
    "\n",
    "I denne notebook vises, hvordan man kan udføre sentiment analyse i Python.\n",
    "\n",
    "Der bruges pakken [`asent`](https://kennethenevoldsen.github.io/asent/installation.html), som virker sammen med `spaCy` til at udføre sentiment analyse.\n",
    "\n",
    "\n",
    "## Hvad er asent?\n",
    "\n",
    "`asent` er en regel-baseret sentiment model, der bygger på en dictionary af ord, som er kategoriseret som hhv. positive og negative ord. Ordene er samtidig givet en polaritetsscore for, hvor positive og negative de er.\n",
    "\n",
    "Modellen virker ved, at den sammenholder ord i et stykke tekst med ordene i dictionary og giver tekststykket en samlet polaritetsscore for, hvor positivt eller negativt det er.\n",
    "\n",
    "\n",
    "### Lexicon\n",
    "\n",
    "Da `asent` er en dictionary-model, skal den bruge en oversigt over ord samt deres tilknyttede score (kaldes ofte \"lexicon\"). De lexicons, som asent bruger, kan findes på pakkens github-repository: [https://github.com/KennethEnevoldsen/asent/tree/main/src/asent/lexicons](https://github.com/KennethEnevoldsen/asent/tree/main/src/asent/lexicons).\n",
    "\n",
    "*Bemærk:* `asent` genbruger lexicons fra andre sentiment modeller ([afinn](https://github.com/fnielsen/afinn) og [sentida](https://github.com/Guscode/Sentida)).\n",
    "\n",
    "\n",
    "### asent i kombination med `spaCy` \n",
    "\n",
    "`asent` virker i kombination med en `spaCy` sprogmodel/pipeline. På den måde kan modellen tage højde for sætningskonstruktion og negationer (fx \"ikke glad\") for at udregne mere præcise polaritetsscores.\n",
    "\n",
    "`spaCy` modeller er altid et *pipeline*:\n",
    "\n",
    "![pipeline](https://spacy.io/images/pipeline-design.svg)\n",
    "\n",
    "Det betyder, at når et stykke tekst behandles med `spaCy` (fx `doc = nlp(text)`), så tages tekststykket igennem flere processer (tokenizer, part-of-speech tagger, dependency parser osv.) - ligesom et samlebånd på en fabrik. \n",
    "\n",
    "Man kan altid tilføje flere led i et `spaCy` pipeline - altså endnu en ting, som teksten skal igennem på samlebåndet. Det er den måde, som man bruger `asent`, da den skal bruge information tidligere i pipelinet for at udføre sentiment analysen af teksten. \n",
    "\n",
    "Resultatet af sentiment analysen for tekststykket tilgås som attributes for doc-objektet (som man ellers tilgår attributes fra et `spaCy` doc-objekt).\n",
    "\n",
    "\n",
    "## Dictionary modeller vs. trænede modeller (neurale netværk, transformers etc.)\n",
    "\n",
    "Dictionary modeller (herunder modeller til sentiment analyse) er efterhånden overgået af præ-trænede maskinlæringsmodeller baseret på transformers eller anden neural netværksarkitektur. Fordelene ved en dictionary model er dog, at det altid er fuldstændig gennemskueligt, hvorfor modellen har analyseret teksten, som den har. Præ-trænede modeller er efterhånden mere nøjagtige, men det er meget vanskeligere for os at spørge modellen, hvorfor den har nået frem til et specifikt resultat (\"black-box\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brug af `asent`\n",
    "\n",
    "I det følgende vises, hvordan `asent` bruges på dansk på enkelte tekststykker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indlæser pakker\n",
    "import spacy\n",
    "import asent\n",
    "\n",
    "#python -m spacy download da_core_news_lg\n",
    "\n",
    "# indlæs spacy sprogmodel og pipeline\n",
    "nlp = spacy.load('da_core_news_lg') # sprogmodel skal være hentet inden, at den kan indlæses\n",
    "\n",
    "# tilføj sentiment model til pipeline\n",
    "nlp.add_pipe('asent_da_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`asent` er nu en del af et `spaCy` pipeline (lagret i `nlp`). Vi er nu klar til at analysere et stykke tekst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jeg kan lide grøn peber på varm leverpostej\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blot at køre teksten gennem pipeline giver ikke noget output, men alle de dele, som er blevet anlayseret og tilføjet teksten på samlebåndet (pipeline), kan nu tilgås i doc-objektet.\n",
    "\n",
    "`asent` giver ikke blot én samlet score for hele teksten, men analyserer i stedet sætninger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence._.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`asent` tildeler både en score for hhv. negativ (`neg`), neutralt (`neu`) og positivt (`pos`). Derudover gives et samlet mål for polaritet (`compound`). Alle scores er normaliseret og rangerer fra -1 til 1. En `compound` score over 0 indikerer derfor overvejende positiv sætning, og under 0 en overvejende negativ sætning.\n",
    "\n",
    "Denne sætning vurderes overvejende positivt (0.4588). \n",
    "\n",
    "### Polaritet på token-niveau\n",
    "\n",
    "Vi kan inspicere resultatet yderligere ved at se, hvordan de enkelte ord/tokens er vurderet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token._.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her ses at ordene \"lide\" og \"varm\" vurderes positive (polarity > 0).\n",
    "\n",
    "`polarity` scores er ikke normaliseret, men er omregninger af de oprindelige tildelte værdier for ordene. De oprindelige tildelte værdier omtales deres \"valence\" og kan ligeledes tilgås:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, \"\\t\", token._.valence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her er der ingen forskel mellem valence og polarity for ordene. Dette fordi at sætningskonstruktionen ikke lægger op til, at ordene skal vægtes anderledes (se andet eksempel længere nede)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisering af udregning\n",
    "\n",
    "`asent` indeholder også visualiseringsfunktioner til at inspicere sentiment analysen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asent.visualize(doc, style=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asent.visualize(doc, style=\"analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negation og tekster med flere udtryk\n",
    "\n",
    "Da `asent` benytter `spaCy` pipeline, kan den tage højde for sætningskonstruktion. I det nedenstående bruges `asent` på en sætning med negation og flere udtryk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jeg kan ikke lide grøn peber på varm leverpostej. Jeg kan bedre lide rød peber.\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asent.visualize(doc, style=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her ses hvordan `asent` behandler ordet \"lide\" forskelligt alt efter kontekst. Da det negeres i første sætning, tildeles det en negativ polaritet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asent.visualize(doc[:9], style=\"analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asent.visualize(doc[10:], style=\"analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence._.polarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
